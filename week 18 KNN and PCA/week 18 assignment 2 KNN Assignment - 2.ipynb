{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sol 1\n",
    "\n",
    "# The main difference between the Euclidean distance metric and the Manhattan distance metric lies in how they measure distance between points in a multidimensional space:\n",
    "\n",
    "    # 1. Euclidean Distance: It is the straight-line distance between two points in Euclidean space. In simple terms, it's the length of the shortest path between two points. Mathematically, it's represented as the square root of the sum of the squared differences between corresponding coordinates of two points.\n",
    "    #     Euclidean Distance = square root of ((x₁ - x₂)² + (y₁ - y₂)² + ... + (z₁ - z₂)²)\n",
    "\n",
    "    # 2. Manhattan Distance: It is the sum of the absolute differences of the coordinates between two points. It's called Manhattan distance because it's akin to how you would navigate on a grid-like Manhattan street grid. Mathematically, it's represented as the sum of the absolute differences between corresponding coordinates of two points.\n",
    "    #     Manhattan Distance = |x₁ - x₂| + |y₁ - y₂| + ... + |z₁ - z₂|\n",
    "\n",
    "# Regarding the performance of a KNN classifier or regressor:\n",
    "\n",
    "# - Euclidean Distance often works well when the data is spread out and the dimensions are independent of each other. It gives more weight to differences in large values.\n",
    "# - Manhattan Distance is often better when the dimensions have different units or when the cost of moving horizontally and vertically is different. It is less influenced by outliers compared to Euclidean distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sol 2\n",
    "\n",
    "# Choosing the optimal value of ( k ) for a KNN classifier or regressor is crucial as it directly impacts the model's performance. Here are some techniques commonly used to determine the optimal ( k ) value:\n",
    "        \n",
    "    # 1. Cross-Validation: Use k-fold cross-validation to evaluate performance for different \\( k \\) values and choose the one with the best average performance.\n",
    "    \n",
    "    # 2. Grid Search: Exhaustively search a range of \\( k \\) values and select the one with the best performance.\n",
    "    \n",
    "    # 3. Elbow Method: Plot performance against \\( k \\) values and choose the point where performance stabilizes.\n",
    "    \n",
    "    # 4. Leave-One-Out Cross-Validation (LOOCV): Train the model with all but one data point and test on the left-out point, repeating for each data point. Select the \\( k \\) with the best average performance.\n",
    "    \n",
    "    # 5. Use Domain Knowledge: Apply expertise to estimate a reasonable range of \\( k \\) values based on data characteristics.\n",
    "    \n",
    "    # 6. Validation Set: Split data into training, validation, and test sets. Tune \\( k \\) on the validation set and evaluate final performance on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sol 3\n",
    "\n",
    "# The choice of distance metric significantly impacts the performance of a KNN classifier or regressor. Here's how:\n",
    "\n",
    "# 1. Euclidean Distance: \n",
    "    # Works well when the data is spread out and the dimensions are independent of each other.\n",
    "    # Gives more weight to differences in large values.\n",
    "    # Sensitive to the scale and magnitude of features.\n",
    "    # Tends to perform better when the underlying data distribution is continuous and follows a Gaussian distribution.\n",
    "\n",
    "# 2. Manhattan Distance: \n",
    "    # Suitable when the dimensions have different units or when the cost of moving horizontally and vertically is different.\n",
    "    # Less influenced by outliers compared to Euclidean distance.\n",
    "    # Effective for high-dimensional data with sparse features.\n",
    "    # Better for data with a grid-like structure or when dealing with categorical variables.\n",
    "\n",
    "# In what situations might we choose one distance metric over the other--\n",
    "\n",
    "    # Euclidean Distance: \n",
    "    # Use when features are continuous and have similar scales.\n",
    "    # Suitable for data where relationships between points can be represented well in a Cartesian coordinate system.\n",
    "    # Commonly chosen as the default distance metric.\n",
    "\n",
    "    # Manhattan Distance: \n",
    "    # Prefer when dealing with features of different units or when the underlying space has a grid-like structure.\n",
    "    # Effective for data with many categorical features.\n",
    "    # Works better when the data distribution is non-Gaussian or non-spherical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sol 4\n",
    "\n",
    "# Common hyperparameters in KNN classifiers and regressors:\n",
    "\n",
    "    # 1. (k): Number of nearest neighbors, affecting model complexity and generalization.\n",
    "    # 2. Distance Metric: Method for calculating distances, influencing similarity measurement.\n",
    "    # 3. Weights: Determines neighbor influence (uniform or distance-based).\n",
    "    # 4. Algorithm: Computation method for nearest neighbors.\n",
    "\n",
    "# Tuning these hyperparameters:\n",
    "\n",
    "    # 1. Grid Search: Evaluate hyperparameter combinations using cross-validation.\n",
    "    # 2. Random Search: Randomly sample hyperparameters for evaluation.\n",
    "    # 3. Cross-Validation: Assess performance across various hyperparameter configurations.\n",
    "    # 4. Domain Knowledge: Use insights about the data and problem domain.\n",
    "    # 5. Automated Optimization: Employ techniques like Bayesian optimization or genetic algorithms.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  sol 5\n",
    "\n",
    "# The size of the training set significantly impacts KNN classifier or regressor performance:\n",
    "\n",
    "# 1. Small Training Set:\n",
    "    # May lead to overfitting and high variance.\n",
    "    # Decision boundaries could be overly sensitive to noise.\n",
    "\n",
    "# 2. Large Training Set:\n",
    "    # Provides more representative samples.\n",
    "    # Helps reduce overfitting and creates smoother decision boundaries.\n",
    "\n",
    "# Optimizing training set size:\n",
    "\n",
    "# 1. Cross-Validation: Evaluate performance across different sizes to find optimal performance.\n",
    "# 2. Learning Curves: Plot performance against training set size to identify plateauing.\n",
    "# 3. Incremental Learning: Train on smaller subsets sequentially for resource efficiency.\n",
    "# 4. Data Augmentation: Create additional samples through transformations.\n",
    "# 5. Feature Selection/Reduction: Reduce dimensionality to mitigate the curse of dimensionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sol 6\n",
    "\n",
    "# Using KNN as a classifier or regressor comes with several potential drawbacks:\n",
    "\n",
    "    # 1. Computational Complexity: High computational cost, especially with large datasets or high dimensions.\n",
    "    # 2. Memory Usage: Requires storing entire training set in memory, impractical for large datasets.\n",
    "    # 3. Sensitive to Irrelevant Features: All features are considered equally, leading to sensitivity to irrelevant or noisy features.\n",
    "    # 4. Curse of Dimensionality: Performance degradation in high-dimensional spaces.\n",
    "    # 5. Need for Optimal  k: Choice of (k) significantly affects performance.\n",
    "\n",
    "# Ways to overcome these drawbacks:\n",
    "\n",
    "    # 1. Dimensionality Reduction: Use techniques like PCA to reduce dimensionality.\n",
    "    # 2. Feature Scaling: Normalize or standardize features.\n",
    "    # 3. Distance Metric Selection: Choose appropriate distance metric.\n",
    "    # 4. Algorithmic Improvements: Use approximate nearest neighbor algorithms or tree-based methods.\n",
    "    # 5. Cross-Validation: Find optimal ( k ) value through cross-validation.\n",
    "    # 6. Ensemble Methods: Combine multiple KNN models or use ensemble methods.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
