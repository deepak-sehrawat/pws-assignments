{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sol 1\n",
    "# Min-Max scaling, also known as feature scaling or normalization, is a data preprocessing technique used to transform numerical features in a dataset to a common scale. \n",
    "# The goal is to bring all the features within a specific range, typically between 0 and 1, while preserving the relative relationships and distributions of the original data. \n",
    "# This scaling method can be particularly useful when working with machine learning algorithms that are sensitive to the magnitude of input features, such as gradient descent-based optimization algorithms.\n",
    "\n",
    "# The Min-Max scaling formula for a feature (X) is as follows:\n",
    "\n",
    "# X_normalized = (X - X_min)/(X_max - X_min)\n",
    "\n",
    "# Where:\n",
    "# - (X_{normalized}) is the scaled value of the feature (X).\n",
    "# - (X_{min}) is the minimum value of feature (X) in the dataset.\n",
    "# - (X_{max}) is the maximum value of feature (X) in the dataset.\n",
    "\n",
    "# Here's an example to illustrate how Min-Max scaling is applied:\n",
    "\n",
    "# Suppose you have a dataset of temperatures recorded in Fahrenheit and you want to scale these temperatures to a range between 0 and 1 using Min-Max scaling.\n",
    "\n",
    "# Original dataset:\n",
    "# - Temperature 1: 32°F\n",
    "# - Temperature 2: 68°F\n",
    "# - Temperature 3: 104°F\n",
    "# - Temperature 4: 50°F\n",
    "\n",
    "# Step 1: Find the minimum and maximum values of the temperatures in the dataset.\n",
    "\n",
    "# - (X_{min}} = 32) (the minimum temperature)\n",
    "# - (X_{max}} = 104) (the maximum temperature)\n",
    "\n",
    "# Step 2: Apply the Min-Max scaling formula to each temperature:\n",
    "\n",
    "# - Temperature 1 (32°F):\n",
    "#   X_{normalized} =(32 - 32)/(104 - 32)\n",
    "\n",
    "# - Temperature 2 (68°F):\n",
    "#   X_{normalized} = (68 - 32)/(104 - 32)\n",
    "\n",
    "# - Temperature 3 (104°F):\n",
    "#   X_{normalized} = (104 - 32)/(104 - 32)\n",
    "\n",
    "# - Temperature 4 (50°F):\n",
    "#   X_{normalized} =  (55 - 32)/(104 - 32)\n",
    "\n",
    "# After Min-Max scaling, your scaled dataset will look like this:\n",
    "\n",
    "# - Temperature 1: 0.0\n",
    "# - Temperature 2: 0.529\n",
    "# - Temperature 3: 1.0\n",
    "# - Temperature 4: 0.274\n",
    "\n",
    "# Now, all the temperatures are within the range of 0 to 1, which can be beneficial when training machine learning models, as it helps avoid issues related to feature magnitudes that may affect the performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sol 2\n",
    "# The Unit Vector technique, also known as \"Normalization\" or \"L2 Normalization,\" is another data preprocessing method used to scale numerical features. Unlike Min-Max scaling, which scales features to a specific range (usually between 0 and 1), Unit Vector scaling scales features to have a magnitude of 1 while preserving their direction. This technique is commonly used in machine learning when you want to emphasize the direction of the data vectors rather than their magnitude.\n",
    "\n",
    "# Here's how Unit Vector scaling works for a feature (X):\n",
    "\n",
    "# [X_{normalized} = {X}/{|X|}]\n",
    "\n",
    "# Where:\n",
    "# - (X_{normalized}) is the scaled value of the feature (X).\n",
    "# - (X) is the original feature value.\n",
    "# - (|X|) is the magnitude (Euclidean norm) of the feature vector (X) in a multi-dimensional space. \n",
    "# It's calculated as (sqrt{X_1^2 + X_2^2 + ldots + X_n^2}), where (X_1, X_2, ldots, X_n) are the components of the feature vector.\n",
    "\n",
    "# Key differences between Unit Vector scaling and Min-Max scaling:\n",
    "\n",
    "# 1. Range:\n",
    "#    - Min-Max scaling constrains the scaled values of features to a specific range (e.g., between 0 and 1).\n",
    "#    - Unit Vector scaling does not impose a specific range; instead, it ensures that the magnitude of each feature vector is 1.\n",
    "\n",
    "# 2. Preservation of relative relationships:\n",
    "#    - Min-Max scaling preserves the relative relationships and distributions of the original data within the chosen range.\n",
    "#    - Unit Vector scaling preserves the relative directions of the data vectors while making them all the same magnitude (1).\n",
    "\n",
    "# Let's illustrate Unit Vector scaling with an example:\n",
    "\n",
    "# Suppose you have a dataset of two-dimensional vectors (features) and you want to apply Unit Vector scaling to them:\n",
    "\n",
    "# Original dataset:\n",
    "# - Vector 1: ([3, 4])\n",
    "# - Vector 2: ([1, 2])\n",
    "# - Vector 3: ([6, 8])\n",
    "\n",
    "# Step 1: Calculate the magnitude of each vector:\n",
    "\n",
    "# - Magnitude of Vector 1: (| [3, 4] | = sqrt{3^2 + 4^2} = 5)\n",
    "# - Magnitude of Vector 2: (| [1, 2] | = sqrt{1^2 + 2^2} = sqrt{5})\n",
    "# - Magnitude of Vector 3: (| [6, 8] | = sqrt{6^2 + 8^2} = 10)\n",
    "\n",
    "# Step 2: Apply Unit Vector scaling to each vector:\n",
    "\n",
    "# - Normalized Vector 1:\n",
    "#   ({[3, 4]}/{5} = [{3}/{5}, {4}/{5}])\n",
    "\n",
    "# - Normalized Vector 2:\n",
    "#   ({[1, 2]}/{sqrt{5}} = [{1}/{sqrt{5}}, {2}/{sqrt{5}}])\n",
    "\n",
    "# - Normalized Vector 3:\n",
    "#   ({[6, 8]}/{10} = [{6}/{10}, {8}/{10}] = [{3}/{5}, {4},{5}])\n",
    "\n",
    "# After Unit Vector scaling, all vectors have a magnitude of 1, but their relative directions remain the same. In this example, you can see that both Vector 1 and Vector 3, which have the same direction, are scaled to the same unit vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sol 3\n",
    "# PCA (Principal Component Analysis) is a statistical technique used for dimensionality reduction. It identifies the most important patterns in high-dimensional data and represents them in a lower-dimensional space, retaining as much information as possible.\n",
    "\n",
    "# Example:\n",
    "# Suppose you have a dataset with multiple features like age, income, and education. PCA can help reduce these features to a smaller set of uncorrelated variables (principal components) that capture most of the data's variability. This simplifies analysis and visualization without losing critical information.\n",
    "\n",
    "# let's illustrate PCA with an example:\n",
    "\n",
    "# Suppose you have a dataset of 2D points in a high-dimensional space. Each data point has two features: x and y coordinates. You want to apply PCA to reduce the dimensionality of the data from 2D to 1D.\n",
    "\n",
    "# Original dataset:\n",
    "\n",
    "# Data Point 1: [1, 2]\n",
    "# Data Point 2: [2, 3]\n",
    "# Data Point 3: [3, 4]\n",
    "# Data Point 4: [4, 5]\n",
    "\n",
    "# Step 1: Standardize the data (mean-centering):\n",
    "\n",
    "# Calculate the mean of each feature:\n",
    "\n",
    "# Mean of x: ((1+2+3+4)/4 = 2.5)\n",
    "# Mean of y: ((2+3+4+5)/4 = 3.5)\n",
    "\n",
    "# Subtract the means from each data point to center the data:\n",
    "\n",
    "# Centered Data Point 1: [-1.5, -1.5]\n",
    "# Centered Data Point 2: [-0.5, -0.5]\n",
    "# Centered Data Point 3: [0.5, 0.5]\n",
    "# Centered Data Point 4: [1.5, 1.5]\n",
    "# Step 2: Compute the covariance matrix:\n",
    "# The covariance matrix for the centered data is calculated as follows:\n",
    "\n",
    "# [Covariance matrix = {bmatrix}\n",
    "# 2.25 & 2.25 \n",
    "# 2.25 & 2.25\n",
    "# {bmatrix}]\n",
    "\n",
    "# Step 3: Calculate eigenvalues and eigenvectors:\n",
    "# Solving for the eigenvalues and eigenvectors of the covariance matrix:\n",
    "\n",
    "# Eigenvalue 1: 4.5 (associated with eigenvector [1, 1])\n",
    "# Eigenvalue 2: 0.0 (associated with eigenvector [-1, 1])\n",
    "# Step 4: Sort eigenvalues and eigenvectors:\n",
    "# Sort in descending order:\n",
    "\n",
    "# Eigenvalue 1: 4.5\n",
    "# Eigenvalue 2: 0.0\n",
    "# Corresponding eigenvectors:\n",
    "\n",
    "# Eigenvector 1: [1, 1]\n",
    "# Eigenvector 2: [-1, 1]\n",
    "# Step 5: Choose the number of components:\n",
    "# In this case, we want to reduce the dimensionality to 1D while retaining most of the variance. Since the first eigenvalue explains all the variance, we'll choose only the first principal component.\n",
    "\n",
    "# Step 6: Project the data:\n",
    "# Project the centered data onto the first principal component (Eigenvector 1):\n",
    "\n",
    "# Projected Data Point 1: [-1.5, -1.5] · [1, 1] = -3\n",
    "# Projected Data Point 2: [-0.5, -0.5] · [1, 1] = -1\n",
    "# Projected Data Point 3: [0.5, 0.5] · [1, 1] = 1\n",
    "# Projected Data Point 4: [1.5, 1.5] · [1, 1] = 3\n",
    "\n",
    "# Now, we have successfully reduced the dimensionality of the data from 2D to 1D using PCA while preserving most of the variance in the dataset. The projected data points along the first principal component capture the essential variation in the original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sol 4\n",
    "# PCA (Principal Component Analysis) is a technique often used for feature extraction in machine learning and data analysis. \n",
    "# Feature extraction is a process where new features, called derived features or components, are created from the original features in a dataset. \n",
    "# The goal is to reduce dimensionality, capture essential information, or create a more meaningful representation of the data.\n",
    "\n",
    "# Here's how PCA is related to feature extraction and how it can be used for this purpose:\n",
    "\n",
    "    # Dimensionality Reduction: PCA is primarily employed for dimensionality reduction. It identifies a set of orthogonal (uncorrelated) linear combinations of the original features, known as principal components. These principal components are sorted by the amount of variance they capture, with the first few components explaining the most variance.\n",
    "\n",
    "    # Feature Extraction: PCA serves as a feature extraction method because the principal components can be seen as new features that summarize the original data. These components are a linear combination of the original features, and each component represents a particular pattern or direction in the data.\n",
    "\n",
    "    # Reducing Redundancy: PCA helps in reducing redundancy among features. When applied to high-dimensional data, it often reveals that much of the variance can be explained by a smaller number of principal components. This means you can retain only a subset of these components as your new features, effectively reducing dimensionality.\n",
    "\n",
    "    # Information Retention: PCA allows you to choose how many principal components (new features) to retain based on the desired level of information retention. By selecting a smaller number of components, you can represent the data with fewer features while preserving most of its variance.\n",
    "\n",
    "# Let's illustrate this concept with an example:\n",
    "\n",
    "# Suppose you have a dataset with three features: height, weight, and age of individuals. You want to perform feature extraction using PCA to reduce the dimensionality while retaining most of the information.\n",
    "\n",
    "# Original dataset (with three features):\n",
    "\n",
    "# Person 1: [180 cm, 75 kg, 30 years]\n",
    "# Person 2: [160 cm, 55 kg, 25 years]\n",
    "# Person 3: [175 cm, 70 kg, 28 years]\n",
    "# Steps:\n",
    "\n",
    "# Standardize the data (if needed) to have zero mean and unit variance for each feature.\n",
    "# Apply PCA to the standardized data.\n",
    "# Calculate the principal components and their corresponding eigenvalues.\n",
    "\n",
    "# Suppose PCA reveals that the first principal component captures 90% of the variance, the second component captures 8%, and the third component captures only 2%.\n",
    "\n",
    "# To perform feature extraction, you can choose to retain only the first two principal components (features) since they capture the most significant variance. This reduces your dataset from three original features to just two derived features.\n",
    "\n",
    "# New dataset (with two derived features):\n",
    "\n",
    "# Person 1: [1.5, 0.5]\n",
    "# Person 2: [-1.5, -0.5]\n",
    "# Person 3: [0.0, 0.0]\n",
    "\n",
    "# Now, we have successfully used PCA for feature extraction, reducing the dimensionality while preserving most of the important information in the data. The two derived features represent the essential patterns in the original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(    price  rating  delivery_time\n",
       " 0     580       4             33\n",
       " 1     852       4             58\n",
       " 2     895       5             27\n",
       " 3     931       7             11\n",
       " 4     915       6             52\n",
       " ..    ...     ...            ...\n",
       " 95    562       6             38\n",
       " 96    555       4             54\n",
       " 97    703       6             23\n",
       " 98    933       7             43\n",
       " 99    236       9             25\n",
       " \n",
       " [100 rows x 3 columns],\n",
       "            0         1         2\n",
       " 0   0.469388  0.444444  0.559140\n",
       " 1   0.979592  0.444444  0.851613\n",
       " 2   0.346939  0.555556  0.897849\n",
       " 3   0.020408  0.777778  0.936559\n",
       " 4   0.857143  0.666667  0.919355\n",
       " ..       ...       ...       ...\n",
       " 95  0.571429  0.666667  0.539785\n",
       " 96  0.897959  0.444444  0.532258\n",
       " 97  0.265306  0.666667  0.691398\n",
       " 98  0.673469  0.777778  0.938710\n",
       " 99  0.306122  1.000000  0.189247\n",
       " \n",
       " [100 rows x 3 columns])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sol 5\n",
    "# for price\n",
    "\n",
    "    # Find the minimum and maximum values of the price feature in your dataset.\n",
    "    # Apply the Min-Max scaling formula to each price value: \n",
    "\n",
    "    # [Price_{scaled}} = {Price - {min(Price)}}/{{max(Price)} - {min(Price)}}]\n",
    "\n",
    "    # The scaled price values will now range between 0 (for the minimum price) and 1 (for the maximum price).\n",
    "\n",
    "# Rating:\n",
    "\n",
    "    # If your rating feature has a fixed range (e.g., ratings on a 1-5 scale), you may choose not to scale it further, as it's already on a bounded scale.\n",
    "    # If the rating feature has a different range, you can still apply Min-Max scaling to normalize it between 0 and 1.\n",
    "\n",
    "    # For example, if ratings range from 0 to 10: \n",
    "\n",
    "    # [Rating_{{scaled}} ={Rating}/{10}] #this is fast way to do this \n",
    "    # This ensures that the scaled ratings also fall within the 0 to 1 range.\n",
    "\n",
    "# Delivery Time:\n",
    "\n",
    "    # Find the minimum and maximum values of the delivery time feature :\n",
    "\n",
    "    # Apply Min-Max scaling to each delivery time value: \n",
    "\n",
    "    # [DeliveryTime_{{scaled}} = {DeliveryTime - {min(DeliveryTime)}}/{{max(DeliveryTime)} - {min(DeliveryTime)}}]\n",
    "\n",
    "    # The scaled delivery time values will now be within the 0 to 1 range.\n",
    "\n",
    "\n",
    "# after changing this into min max scaler convert them into datset \n",
    "\n",
    "\n",
    "#### this is the python cod for doing the same by using the library ####\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.random.seed(100)\n",
    "#Lets create the dataset for the project \n",
    "data={\"price\":np.random.randint(60,1000,size=100),\n",
    "      \"rating\":np.random.randint(0.0,10,size=100),\n",
    "      \"delivery_time\":np.random.randint(10,60,size=100)}\n",
    "df=pd.DataFrame(data=data)\n",
    "\n",
    "#lets apply min-max scaling\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler=MinMaxScaler()\n",
    "scaler.fit(df[[\"delivery_time\",\"rating\",\"price\"]])\n",
    "new_df=scaler.transform(df[[\"delivery_time\",\"rating\",\"price\"]])\n",
    "new_df=pd.DataFrame(data=new_df)\n",
    "df,new_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sol 6\n",
    "# Using PCA (Principal Component Analysis) for dimensionality reduction in the context of predicting stock prices can be a valuable approach, especially when dealing with a dataset that contains many features. \n",
    "\n",
    "#steps for doing the PCA for predicting the stock prices \n",
    "\n",
    "    # Data Preprocessing:\n",
    "\n",
    "    # Begin by preprocessing the dataset. This includes handling missing values, normalizing features (if necessary), and ensuring that all features are on a consistent scale. Since you're dealing with financial data, it's crucial to preprocess it carefully.\n",
    "\n",
    "    # Standardization (Optional):\n",
    "\n",
    "    # Depending on the nature of your financial features, you may want to standardize them (subtract the mean and divide by the standard deviation) to ensure that features with different units or scales do not dominate the PCA process. Standardization can help when you have features with different magnitudes, such as stock prices and market capitalization.\n",
    "   \n",
    "    # Apply PCA:\n",
    "\n",
    "    # Once the data is preprocessed, apply PCA to the dataset. PCA identifies a set of uncorrelated linear combinations of the original features called principal components. These components capture the most significant variations in the data.\n",
    "    \n",
    "    # Determine the Number of Components:\n",
    "\n",
    "    # You'll need to decide how many principal components to retain. Typically, this is done based on the amount of variance explained. You can plot the cumulative explained variance ratio as a function of the number of components and choose a threshold (e.g., retaining 95% of the variance).\n",
    "    \n",
    "    # Select Principal Components:\n",
    "\n",
    "    # After determining the number of components, select the top N principal components that account for the chosen percentage of the total variance. These components will replace the original features in your dataset.\n",
    "    \n",
    "    # Feature Transformation:\n",
    "\n",
    "    # Transform your original dataset by projecting it onto the selected principal components. This creates a new dataset with reduced dimensionality. The transformed dataset will have N features, where N is the number of selected principal components.\n",
    "    \n",
    "    # Model Building:\n",
    "\n",
    "    # With the reduced-dimensional dataset, you can proceed to build your stock price prediction model. You may use regression techniques, time series models, or machine learning algorithms, depending on the specific problem and dataset.\n",
    "    \n",
    "    # Model Evaluation:\n",
    "\n",
    "    # Evaluate the performance of your model using appropriate evaluation metrics (e.g., Mean Absolute Error, Root Mean Squared Error) and consider cross-validation to ensure the model's robustness.\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-1.0, -0.5789473684210527, -0.052631578947368474, 0.4736842105263157, 1.0]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sol 7\n",
    "values= [1, 5, 10, 15, 20]\n",
    "a_v=np.array(values)\n",
    "max=np.max(a_v)\n",
    "min=np.min(a_v)\n",
    "mean=np.mean(a_v)\n",
    "\n",
    "new_min=-1\n",
    "new_max=1\n",
    "# min=mean\n",
    "\n",
    "result=[((i-min)/(max-min)*(new_max-new_min))+new_min for i in values]\n",
    "result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.         -0.57894737 -0.05263158  0.47368421  1.        ]\n"
     ]
    }
   ],
   "source": [
    "# sol 8\n",
    "# The number of principal components to retain in a PCA analysis for the given dataset [height, weight, age, gender, blood pressure] would depend on your specific goals and the trade-off between dimensionality reduction and information preservation.\n",
    "\n",
    "# Typically, we follow these steps:\n",
    "\n",
    "\n",
    "# 1. Data Preprocessing:\n",
    "#  Clean the data, handle missing values, and encode categorical features like \"gender\" (e.g., one-hot encoding).\n",
    "\n",
    "# 2. Standardization:\n",
    "#  Standardize the numerical features (height, weight, age, blood pressure) to have a mean of 0 and a standard deviation of 1. This step ensures that all features contribute equally to PCA.\n",
    "\n",
    "# 3. Apply PCA:\n",
    "#  Perform PCA on the standardized dataset.\n",
    "\n",
    "# 4. Determine the Number of Components:\n",
    "#  You can choose the number of principal components based on various criteria:\n",
    "\n",
    "\n",
    "#    - Explained Variance:\n",
    "#     Plot the explained variance ratio as a function of the number of components. You might decide to retain enough components to capture a certain percentage of the total variance (e.g., 95%).\n",
    "\n",
    "#    - Domain Knowledge:\n",
    "#     Consider the requirements of your problem and whether reducing dimensionality is essential for simplicity or interpretability.\n",
    "\n",
    "# ******Ultimately, the number of components you choose to retain will depend on your specific analysis objectives.****** \n",
    "# It's a trade-off between reducing dimensionality and preserving information. You can experiment with different numbers of components and evaluate their impact on your analysis or predictive model to make an informed decision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
