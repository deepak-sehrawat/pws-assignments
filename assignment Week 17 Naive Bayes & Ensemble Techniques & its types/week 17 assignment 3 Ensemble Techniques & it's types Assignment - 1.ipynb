{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sol 1\n",
    "# In machine learning, an ensemble technique refers to the process of combining multiple models to improve the performance of the overall system.\n",
    "\n",
    "# There are four main popular ensemble techniques:\n",
    "\n",
    "# 1. Bagging (Bootstrap Aggregating): Train multiple instances of the same algorithm on different subsets of data, often sampled with replacement. Combine predictions by averaging or majority voting.\n",
    "\n",
    "# 2. Boosting: Sequentially train models, with each subsequent model focusing more on instances misclassified by previous ones. Examples: AdaBoost, Gradient Boosting.\n",
    "\n",
    "# 3. Random Forest: Ensemble method combining multiple decision trees. Each tree trained on random subsets of features and data. Final prediction made by averaging or majority voting.\n",
    "\n",
    "# 4. Stacking: Train diverse base models, then combine predictions using a meta-learner model to make the final prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sol 2\n",
    "\n",
    "# Reason for using ensemble techniques used in machine learning\n",
    "    \n",
    "# 1. Improved Accuracy: Combining multiple models from different subsets or algorithms reduces overfitting and captures broader patterns for higher accuracy.\n",
    "  \n",
    "# 2. Reduced Overfitting: Ensembles mitigate overfitting by combining diverse models, leading to more generalizable predictions.\n",
    "\n",
    "# 3. Robustness: Aggregating predictions from multiple models smooths out errors, making predictions more reliable in the presence of noise and outliers.\n",
    "\n",
    "# 4. Handles Complex Relationships: Ensembles combine models with different characteristics to better approximate complex relationships in data.\n",
    "\n",
    "# 5. Versatility: Ensemble techniques can be applied to various algorithms, allowing for wide-ranging applications across different domains.\n",
    "\n",
    "# 6. Interpretability: Ensemble methods offer insights into data structure by analyzing contributions of individual models, aiding understanding of feature importance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sol 3\n",
    "# Bagging, is an ensemble learning technique in machine learning. It involves training multiple instances of the same base learning algorithm on different subsets of the training data. \n",
    "# These subsets are typically sampled randomly from the original training data, often with replacement (bootstrap sampling). Each instance, also known as a base model or a weak learner, is trained independently. Then, predictions from these models are combined to make the final prediction. \n",
    "# This combination can be done through averaging (for regression tasks) or majority voting (for classification tasks). \n",
    "# Bagging helps to reduce variance and prevent overfitting by training models on different subsets of data and combining their predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sol 4\n",
    "# Boosting is an ensemble learning technique where base models are trained sequentially, with each subsequent model focusing on instances that previous models misclassified.\n",
    "\n",
    "# It iteratively adjusts the weights of training instances to prioritize difficult-to-classify cases, resulting in a strong learner with improved performance.\n",
    "\n",
    "# Examples of boosting algorithms include AdaBoost (Adaptive Boosting) and Gradient Boosting, which are widely used for classification and regression tasks in machine learning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sol 5\n",
    "\n",
    "# the benefits of using ensemble techniques\n",
    "\n",
    "# 1. Improved Performance: Ensembles enhance prediction accuracy by combining multiple models.\n",
    "\n",
    "# 2. Stability: They're less sensitive to dataset changes due to averaging or voting.\n",
    "\n",
    "# 3. Reduced Bias: Combining different models helps mitigate bias and capture diverse data aspects.\n",
    "\n",
    "# 4. Nonlinear Relationships: Ensembles effectively model complex data patterns, especially nonlinear ones.\n",
    "\n",
    "# 5. Robustness: They handle outliers and noisy data better by averaging or voting.\n",
    "\n",
    "# 6. Versatility: Ensembles can be applied to various algorithms and problem types.\n",
    "\n",
    "# 7. Interpretability: Some methods offer insights into feature importance, aiding in model understanding.\n",
    "\n",
    "# 8. Overfitting Reduction: Ensembles combat overfitting by combining models that overfit differently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sol 6\n",
    "\n",
    "# Ensemble techniques aren't always superior to individual models due to factors like computational resources, data size, interpretability, and model diversity. \n",
    "# In some cases, simpler models might suffice or be more suitable, particularly in scenarios with limited resources, small datasets, or where interpretability is crucial. \n",
    "# It's essential to consider the trade-offs and specific requirements of the problem domain when deciding whether to use an ensemble or a single model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sol 7\n",
    "\n",
    "# To calculate the confidence interval using bootstrap, we follow these steps:\n",
    "\n",
    "# 1. Sampling with Replacement: Generate multiple bootstrap samples by randomly sampling with replacement from the original dataset. Each bootstrap sample has the same size as the original dataset.\n",
    "\n",
    "# 2. Calculate Statistic: For each bootstrap sample, calculate the statistic of interest (e.g., mean, median, standard deviation, etc.).\n",
    "\n",
    "# 3. Compute Confidence Interval: Sort the bootstrap statistics in ascending order. Then, determine the lower and upper bounds of the confidence interval based on the desired confidence level and the distribution of the bootstrap statistics.\n",
    "\n",
    "    # For example, to obtain a 95% confidence interval, we would exclude the lowest and highest 2.5% of the bootstrap statistics, leaving we with the middle 95% as the confidence interval.\n",
    "\n",
    "# 4. Estimate: Finally, use the lower and upper bounds obtained from the bootstrap statistics to define the confidence interval for the original population parameter.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sol 8\n",
    "\n",
    "# Bootstrap is a resampling technique used to estimate the sampling distribution of a statistic or to assess the uncertainty of a parameter estimate. It involves repeatedly sampling from the observed data with replacement to create multiple datasets\n",
    "\n",
    "# steps:-\n",
    "\n",
    "# 1. Sample with Replacement: Create multiple bootstrap samples by randomly selecting observations from the original dataset with replacement.\n",
    "\n",
    "# 2. Compute Statistic: Calculate the statistic of interest (e.g., mean, median) for each bootstrap sample.\n",
    "\n",
    "# 3. Construct Sampling Distribution: Compile the computed statistics to create the bootstrap sampling distribution.\n",
    "\n",
    "# 4. Estimate Uncertainty: Use the bootstrap sampling distribution to estimate uncertainty, typically by calculating confidence intervals around the statistic of interest.\n",
    "\n",
    "# 5. Interpret Results: Interpret the results, considering the estimated uncertainty to make informed decisions or inference.\n",
    "\n",
    "# Bootstrap provides a straightforward way to estimate uncertainty without relying on assumptions about the underlying population distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sol 9\n",
    "# To estimate the 95% confidence interval for the population mean height using bootstrap, we'll follow these steps:\n",
    "\n",
    "    # 1. Bootstrap Resampling: Generate multiple bootstrap samples by randomly sampling with replacement from the observed sample of tree heights.\n",
    "\n",
    "    # 2. Calculate Mean Height: Calculate the mean height for each bootstrap sample.\n",
    "\n",
    "    # 3. Construct Confidence Interval: Determine the lower and upper bounds of the confidence interval using percentiles of the bootstrap sample distribution.\n",
    "\n",
    "# proceed with the calculation:\n",
    "\n",
    "    # 1. Bootstrap Resampling: We'll simulate multiple bootstrap samples by resampling with replacement from the observed sample of tree heights.\n",
    "\n",
    "    # 2. Calculate Mean Height: For each bootstrap sample, we'll calculate the mean height.\n",
    "\n",
    "    # 3. Construct Confidence Interval: We'll use the percentiles of the bootstrap sample distribution to determine the lower and upper bounds of the 95% confidence interval.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95% Confidence Interval for the Population Mean Height (meters): [14.44797668 15.54104409]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "sample_mean_height = 15\n",
    "sample_std_dev = 2\n",
    "\n",
    "\n",
    "num_bootstrap_samples = 1000\n",
    "\n",
    "\n",
    "bootstrap_means = []\n",
    "for _ in range(num_bootstrap_samples):\n",
    "    bootstrap_sample = np.random.normal(sample_mean_height, sample_std_dev, 50)\n",
    "    bootstrap_mean = np.mean(bootstrap_sample)\n",
    "    bootstrap_means.append(bootstrap_mean)\n",
    "\n",
    "\n",
    "confidence_interval = np.percentile(bootstrap_means, [2.5, 97.5])\n",
    "\n",
    "print(\"95% Confidence Interval for the Population Mean Height (meters):\",\n",
    "      confidence_interval)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
